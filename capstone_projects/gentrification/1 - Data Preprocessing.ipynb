{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T13:36:45.006682Z",
     "start_time": "2018-01-17T13:36:37.787819Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sources\n",
    "There are two similar yet slightly different sources for this data. The 2000 data comes from the Decennial United States Census whereas the 2009-2016 data comes from American Community Survey 5-year estimates. Both data sources contain roughly the same features, format, and geographic contraints (census tracts). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T13:36:47.569199Z",
     "start_time": "2018-01-17T13:36:47.475448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected file not processed: ACS_09_5YR_B11001_with_ann.csv\n",
      "Unexpected file not processed: ACS_10_5YR_B11001_with_ann.csv\n",
      "Unexpected file not processed: ACS_11_5YR_B11001_with_ann.csv\n",
      "Unexpected file not processed: ACS_12_5YR_B11001_with_ann.csv\n",
      "Unexpected file not processed: ACS_13_5YR_B11001_with_ann.csv\n",
      "Unexpected file not processed: ACS_14_5YR_B11001_with_ann.csv\n",
      "Unexpected file not processed: ACS_15_5YR_B11001_with_ann.csv\n",
      "Unexpected file not processed: ACS_16_5YR_B11001_with_ann.csv\n",
      "Unexpected file not processed: DEC_00_SF4_QTP10_with_ann.csv\n"
     ]
    }
   ],
   "source": [
    "#system agnostic data path\n",
    "raw_data_dir = os.path.join(os.getcwd(), 'data', 'raw')\n",
    "#empty dict to hold all the variables, this will come in handy later\n",
    "empty_vars = {'income': '', 'education': '', 'race': '',\n",
    "              'rent': '', 'value': '', 'unemployment': ''}\n",
    "#these are the data variable names for the decennial census\n",
    "dec_data_vars = {'P052': 'income', 'QTP20': 'education', 'P007': 'race',\n",
    "                 'H062': 'rent', 'H084': 'value', 'QTP24': 'unemployment'}\n",
    "#these are the data variable names for the ACS 5-year estimates\n",
    "acs_data_vars = {'B19001': 'income', 'S1501': 'education', 'B02001': 'race',\n",
    "                 'B25063': 'rent', 'B25075': 'value', 'S2301': 'unemployment'}\n",
    "#this will hold all the different file names to make reading data easier\n",
    "data_dict = {\n",
    "    '2000': empty_vars.copy(),\n",
    "    '2009': empty_vars.copy(),\n",
    "    '2010': empty_vars.copy(),\n",
    "    '2011': empty_vars.copy(),\n",
    "    '2012': empty_vars.copy(),\n",
    "    '2013': empty_vars.copy(),\n",
    "    '2014': empty_vars.copy(),\n",
    "    '2015': empty_vars.copy(),\n",
    "    '2016': empty_vars.copy()\n",
    "}\n",
    "#loop through raw data files (csv only) and assign each one to proper key\n",
    "for fil in os.listdir(raw_data_dir):\n",
    "    if fil.endswith('.csv'):\n",
    "        fil_spl = fil.split('_')\n",
    "        if fil_spl[3] in acs_data_vars.keys():\n",
    "            data_dict['20'+fil_spl[1]][acs_data_vars[fil_spl[3]]] = os.path.join(raw_data_dir, fil)\n",
    "        elif fil_spl[3] in dec_data_vars.keys():\n",
    "            data_dict['20'+fil_spl[1]][dec_data_vars[fil_spl[3]]] = os.path.join(raw_data_dir, fil)\n",
    "        else:\n",
    "            print('Unexpected file not processed: {}'.format(fil))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relating 2000 and 2010 Census Tracts\n",
    "One complication that arises while working with census data is that tracts change over time as population changes. Tracts generally encompass a population between 2,500 and 8,000 people and are the intended to be a rough equivalent of a neighborhood. As the population grows, shrinks, and moves, some census tracts are split into smaller tracts while others are merged to form a new tract. This is the case between the 2000-2009 data, which use the 2000 census tracts, vs the 2010-2016 data, which use the 2010 census tracts. \n",
    "\n",
    "Thankfully, the Census Bureau tracks this sort of change very carefully and presents the morphology of census tracts in a detailed relationship file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T13:36:52.413012Z",
     "start_time": "2018-01-17T13:36:51.975507Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "geoid_2000 = pd.read_csv(data_dict['2000']['income'], skiprows=1)['Id2']\n",
    "geoid_2010 = pd.read_csv(data_dict['2016']['income'], skiprows=1)['Id2']\n",
    "\n",
    "removed = list(set(geoid_2000) - set(geoid_2010))\n",
    "added = list(set(geoid_2010) - set(geoid_2000))\n",
    "\n",
    "tract_relations = pd.read_csv(os.path.join(os.getcwd(), 'census_tract_shapefile', 'census_tract_relation_file.csv'),\n",
    "                              usecols=[1, 3, 12, 25, 26])\n",
    "tract_relations = tract_relations[tract_relations['county00'].isin([21, 55, 209, 453, 491])]\n",
    "\n",
    "need_to_change = tract_relations[tract_relations['geoid00'].isin(removed)]\n",
    "need_to_change = need_to_change[need_to_change['geoid10'].isin(added)]\n",
    "need_to_change = need_to_change[need_to_change['poppct00'] > 0]\n",
    "need_to_change = need_to_change[need_to_change['poppct10'] > 1]\n",
    "\n",
    "need_to_change.to_csv(os.path.join(os.getcwd(), 'census_tract_shapefile', 'need_to_change.csv'))\n",
    "\n",
    "merged = need_to_change[(need_to_change['poppct00'] > 95) & (need_to_change['poppct10'] < 100)]\n",
    "split = need_to_change[need_to_change['poppct10'] > 95]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T13:36:55.209926Z",
     "start_time": "2018-01-17T13:36:55.178677Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fix_tracts(df):\n",
    "    #the tract for austin-bergstrom airport is missing from 2000 and 2009\n",
    "    #there's no population or housing units in the tract (geoid: 48453980000)\n",
    "    #so it can be filled in with zeros\n",
    "    df.loc[48453980000] = 0\n",
    "    #merged\n",
    "    for geoid in merged['geoid10']:\n",
    "        df.loc[geoid] = df.loc[list(merged['geoid00'])].sum()\n",
    "    df.drop(list(merged['geoid00']), inplace=True)\n",
    "    #split\n",
    "    for row in split.iterrows():\n",
    "        df.loc[row[1]['geoid10']] = round(df.loc[row[1]['geoid00']] * row[1]['poppct00'] / 100, 0)\n",
    "    df.drop(list(split['geoid00'].unique()), inplace=True)\n",
    "    df.astype('int')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T13:36:56.163063Z",
     "start_time": "2018-01-17T13:36:56.116188Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_income(year):\n",
    "    if year == '2000':\n",
    "        usecols = [1, 3, 4, 5, 6, 7, 8, 9, 10, 11, \n",
    "                   12, 13, 14, 15, 16, 17, 18, 19]\n",
    "    else:\n",
    "        usecols=[1, 3, 5, 7, 9, 11, 13, 15, 17, 19,\n",
    "                 21, 23, 25, 27, 29, 31, 33, 35]\n",
    "    income = pd.read_csv(data_dict[year]['income'], skiprows=1, usecols=usecols)\n",
    "    income.index = income.pop('Id2')\n",
    "    income.index.name = 'geoid'\n",
    "    income.columns = ['total', '<10k', '[10k-15k)', '[15k-20k)',\n",
    "                      '[20k-25k)', '[25k-30k)', '[30k-35k)', '[35k-40k)', '[40k-45k)',\n",
    "                      '[45k-50k)', '[50k-60k)', '[60k-75k)', '[75k-100k)', '[100k-125k)',\n",
    "                      '[125k-150k)', '[150k-200k)', '>200k']\n",
    "    if year in ['2000', '2009']:\n",
    "        income = fix_tracts(income)\n",
    "    \n",
    "    return income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T13:36:56.944323Z",
     "start_time": "2018-01-17T13:36:56.772447Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_education(year):\n",
    "    if year == '2000':\n",
    "        usecols = [3, 20, 23, 26, 29, 32, 35, 38, 41, 44, 47, 50, 53]\n",
    "    elif year in ['2009', '2010', '2011', '2012', '2013']:\n",
    "        usecols = [1, 33, 39, 45, 51, 57, 63, 69, 75]\n",
    "    elif year == '2014':\n",
    "        usecols = [1, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "    else:\n",
    "        usecols = [1, 13, 15, 17, 19, 21, 23, 25, 27]\n",
    "    \n",
    "    education = pd.read_csv(data_dict[year]['education'], skiprows=1, usecols=usecols, na_values=['-'])\n",
    "    education.index = education.pop('Id2')\n",
    "    education.index.name = 'geoid'\n",
    "    \n",
    "    if year == '2000':\n",
    "        education.columns = ['total', '<5', '5-8', '9-12', 'high_school', 'some_college_1', \n",
    "                             'some_college_2', 'associate', 'bachelor', 'master', \n",
    "                             'professional', 'doctorate']\n",
    "        education['<9'] = education.pop('<5') + education.pop('5-8')\n",
    "        education['some_college'] = education.pop('some_college_1') + education.pop('some_college_2')\n",
    "        education['graduate'] = education.pop('master') + education.pop('professional') \\\n",
    "                                + education.pop('doctorate')\n",
    "        education = education[['total', '<9', '9-12', 'high_school', 'some_college',\n",
    "                               'associate', 'bachelor', 'graduate']]\n",
    "    \n",
    "    elif year in ['2015', '2016']:\n",
    "        education.columns = ['total', '<9', '9-12', 'high_school', 'some_college',\n",
    "                             'associate', 'bachelor', 'graduate']\n",
    "    \n",
    "    else:\n",
    "        education.columns = ['total', '<9', '9-12', 'high_school', 'some_college',\n",
    "                             'associate', 'bachelor', 'graduate']\n",
    "        education.fillna(0, inplace=True) #not actually missing - Austin-Bergrstrom Intl. Airport\n",
    "        perc_cols = education.columns[1:]\n",
    "        for c in perc_cols:\n",
    "            education[c] = round(education[c] / 100 * education['total'], 0).astype('int')\n",
    "        \n",
    "    if year in ['2000', '2009']:\n",
    "        education = fix_tracts(education)\n",
    "        \n",
    "    return education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T13:36:57.334954Z",
     "start_time": "2018-01-17T13:36:57.288078Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_race(year):\n",
    "    if year == '2000':\n",
    "        usecols = [1, 3, 4]\n",
    "    else:\n",
    "        usecols = [1, 3, 5]\n",
    "    race = pd.read_csv(data_dict[year]['race'], skiprows=1, usecols=usecols)\n",
    "    race.index = race.pop('Id2')\n",
    "    race.index.name = 'geoid'\n",
    "    race.columns = ['total_population', 'white_alone']\n",
    "    \n",
    "    if year in ['2000', '2009']:\n",
    "        race = fix_tracts(race)\n",
    "        \n",
    "    '''\n",
    "    still need to convert to percent (here or next step?)\n",
    "    '''\n",
    "    \n",
    "    return race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T13:36:57.897461Z",
     "start_time": "2018-01-17T13:36:57.866211Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_families(year):\n",
    "    if year == '2000':\n",
    "        usecols = [3, 5, 7]\n",
    "    else:\n",
    "        usecols = [1, 3, 5]\n",
    "    families = pd.read_csv(data_dict[year]['families'], skiprows=1, usecols=usecols)\n",
    "    families.index = families.pop('Id2')\n",
    "    families.index.name = 'geoid'\n",
    "    families.columns = ['total_households', 'family_households']\n",
    "    \n",
    "    if year in ['2000', '2009']:\n",
    "        families = fix_tracts(families)\n",
    "        \n",
    "    '''\n",
    "    still need to convert to percent (here or next step?)\n",
    "    '''\n",
    "    \n",
    "    return families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T13:36:58.709972Z",
     "start_time": "2018-01-17T13:36:58.631846Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_rent(year):\n",
    "    if year == '2000':\n",
    "        usecols = [1, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, \n",
    "                   17, 18, 19, 20, 21, 22, 23, 24, 25]\n",
    "    elif year in ['2009', '2010', '2011', '2012', '2013', '2014']:\n",
    "        usecols = [1, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, \n",
    "                   27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47]\n",
    "    else:\n",
    "        usecols=[1, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, \n",
    "                 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, \n",
    "                 51, 53]\n",
    "    rent = pd.read_csv(data_dict[year]['rent'], skiprows=1, usecols=usecols)\n",
    "    rent.index = rent.pop('Id2')\n",
    "    rent.index.name = 'geoid'\n",
    "    if year in ['2015', '2016']:\n",
    "        rent['>2000'] = rent[rent.columns[21:25]].sum(axis=1)\n",
    "        rent.drop(rent.columns[[21, 22, 23, 24]], axis=1, inplace=True)\n",
    "    \n",
    "    rent.columns = ['total', '<100', '[100-150)', '[150-200)', '[200-250)',\n",
    "                    '[250-300)', '[300-350)', '[350-400)', '[400-450)', '[450-500)', '[500-550)',\n",
    "                    '[550-600)', '[600-650)', '[650-700)', '[700-750)', '[750-800)', '[800-900)',\n",
    "                    '[900-1000)', '[1000-1250)', '[1250-1500)', '[1500-2000)', '>2000']\n",
    "    \n",
    "    if year in ['2000', '2009']:\n",
    "        rent = fix_tracts(rent)\n",
    "    \n",
    "    return rent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T13:36:59.678734Z",
     "start_time": "2018-01-17T13:36:59.584983Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_value(year):\n",
    "    if year == '2000':\n",
    "        usecols = [1, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, \n",
    "                   16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27]\n",
    "    elif year in ['2009', '2010', '2011', '2012', '2013', '2014']:\n",
    "        usecols = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, \n",
    "                   29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51]\n",
    "    else:\n",
    "        usecols = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, \n",
    "                   29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55]\n",
    "    \n",
    "    value = pd.read_csv(data_dict[year]['value'], skiprows=1, usecols=usecols)\n",
    "    value.index = value.pop('Id2')\n",
    "    value.index.name = 'geoid'\n",
    "    \n",
    "    if year in ['2015', '2016']:\n",
    "        value['>1M'] = value[value.columns[25:28]].sum(axis=1)\n",
    "        value.drop(value.columns[[25, 26, 27]], axis=1, inplace=True)\n",
    "    \n",
    "    value.columns = ['total', '<10k', '[10k-15k)', '[15k-20k)', '[20k-25k)', \n",
    "                     '[25k-30k)', '[30k-35k)', '[35k-40k)', '[40k-50k)', '[50k-60k)', '[60k-70k)', \n",
    "                     '[70k-80k)', '[80k-90k)', '[90k-100k)', '[100k-125k)', '[125k-150k)', '[150k-175k)', \n",
    "                     '[175k-200k)', '[200k-250k)', '[250k-300k)', '[300k-400k)', '[400k-500k)', \n",
    "                     '[500k-750k)', '[750k-1M)', '>1M']\n",
    "    \n",
    "    if year in ['2000', '2009']:\n",
    "        value = fix_tracts(value)\n",
    "    \n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T13:37:00.600621Z",
     "start_time": "2018-01-17T13:37:00.553746Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_unemployment(year):\n",
    "    if year == '2000':\n",
    "        usecols = [3, 5, 35]\n",
    "    else:\n",
    "        usecols = [1, 3, 9]\n",
    "    unemployment = pd.read_csv(data_dict[year]['unemployment'], skiprows=1, usecols=usecols, na_values='-')\n",
    "    unemployment.index = unemployment.pop('Id2')\n",
    "    unemployment.index.name = 'geoid'\n",
    "    unemployment.columns = ['total_in_labor_force', 'unemployed']\n",
    "    if year != '2000':\n",
    "        unemployment.fillna(0, inplace=True)\n",
    "        unemployment['unemployed'] = round(unemployment['total_in_labor_force']\\\n",
    "                                           * unemployment['unemployed'] / 100, 0)\n",
    "    \n",
    "    if year in ['2000', '2009']:\n",
    "        unemployment = fix_tracts(unemployment)\n",
    "        \n",
    "    return unemployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-17T13:37:10.882046Z",
     "start_time": "2018-01-17T13:37:01.397509Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = {\n",
    "    '2000': empty_vars.copy(),\n",
    "    '2009': empty_vars.copy(),\n",
    "    '2010': empty_vars.copy(),\n",
    "    '2011': empty_vars.copy(),\n",
    "    '2012': empty_vars.copy(),\n",
    "    '2013': empty_vars.copy(),\n",
    "    '2014': empty_vars.copy(),\n",
    "    '2015': empty_vars.copy(),\n",
    "    '2016': empty_vars.copy()\n",
    "}\n",
    "\n",
    "read_funs = [read_income, read_education, read_race,\n",
    "             read_rent, read_value, read_unemployment]\n",
    "read_vars = ['income', 'education', 'race', \n",
    "             'rent', 'value', 'unemployment']\n",
    "\n",
    "for key in all_data.keys():\n",
    "    for f, v in zip(read_funs, read_vars):\n",
    "        all_data[key][v] = f(key)\n",
    "        \n",
    "pickle_all = open(os.path.join(os.getcwd(), 'data', 'processed', 'all_data.pickle'), 'wb')\n",
    "pickle.dump(all_data, pickle_all)\n",
    "\n",
    "#All the data can now be read with the following two lines of code:\n",
    "#pickle_in = open(os.path.join(os.getcwd(), 'data', 'processed', 'all_data.pickle'), 'rb')\n",
    "#data = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the data\n",
    "Now we'll aggregate the binned data (rent, value, income, education) into indices, convert race and unemployment to percentages, as well as adjust each year for inflation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make a copy of all_data for transforming data \n",
    "#need to use copy.deepcopy() here so that the dataframes inside the dictionary are\n",
    "#copied as well - if you just you all_data.copy(), the dictionary will be copied but\n",
    "#the dataframes will share memory and any modifications to dataframes will be applied \n",
    "#the dataframes in all_data as well\n",
    "trans_data = copy.deepcopy(all_data)\n",
    "#the scorer for monetary variables is the middle value of the bin with an assumed\n",
    "#value for the uppermost bin\n",
    "income_scorer = np.array([5, 12.5, 17.5, 22.5, 27.5, 32.5, 37.5, 42.5, 47.5,\n",
    "                          55, 67.5, 87.5, 112.5, 137.5, 175, 300]) * 1000\n",
    "rent_scorer = np.array([50, 125, 175, 225, 275, 325, 375, 425, 475, 525, 575,\n",
    "                        625, 675, 725, 775, 850, 950, 1125, 1375, 1750, 2500])\n",
    "value_scorer = np.array([5, 12.5, 17.5, 22.5, 27.5, 32.5, 27.5, 45, 55, 65, 75, \n",
    "                         85, 95, 112.5, 137.5, 162.5, 187.5, 225, 275, 350, 450, \n",
    "                         625, 875, 1500]) * 1000\n",
    "#education gets a simple 1-8 score\n",
    "education_scorer = np.arange(1, 8)\n",
    "\n",
    "#to adjust the values to 2016 dollars, the index are adjusted by the \n",
    "#yearly average consumer price index \n",
    "inflation_correction = np.array([1.39422697, 1.11865482, 1.10084296, 1.06686838,\n",
    "                                 1.04536021, 1.03039158, 1.01380104, 1.01263642, \n",
    "                                 1.00000000])\n",
    "\n",
    "for i,year in enumerate(trans_data.keys()):\n",
    "    for key in trans_data[year].keys():\n",
    "        tmp = trans_data[year][key]\n",
    "        #bust out the total population from the race df\n",
    "        if key == 'race':\n",
    "            population = tmp['total_population'].to_frame()\n",
    "        #bust out total in labor force from the unemployment df\n",
    "        if key == 'unemployment':\n",
    "            tmp['employed'] = tmp['total_in_labor_force'] - tmp['unemployed']\n",
    "        #convert columns to percentages of total and drop the total column\n",
    "        tmp = tmp[tmp.columns[1:]].divide(tmp[tmp.columns[0]], axis=0).fillna(0)\n",
    "        #calculate the index (single number to represent binned data)\n",
    "        if len(tmp.columns) > 1:\n",
    "            if key == 'income':\n",
    "                tmp['income_index'] = tmp.dot(income_scorer) * inflation_correction[i]\n",
    "            elif key == 'rent':\n",
    "                tmp['rent_index'] = tmp.dot(rent_scorer) * inflation_correction[i]\n",
    "            elif key == 'value':\n",
    "                tmp['value_index'] = tmp.dot(value_scorer) * inflation_correction[i]\n",
    "            elif key == 'education':\n",
    "                tmp['education_index'] = tmp.dot(education_scorer) \n",
    "        #put the new percentage column back into the dataframe\n",
    "        trans_data[year][key] = tmp[tmp.columns[-1]].copy().to_frame()\n",
    "    #concatenate the summary columns (1 for each dataframe) into a single dataframe\n",
    "    trans_data[year] = pd.concat([trans_data[year][key] for key in trans_data[year].keys()], axis=1)\n",
    "    #add a year column in the first position to be used later\n",
    "    trans_data[year].insert(0, 'year', year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#now we'll save this version of the data\n",
    "pickle_trans = open(os.path.join(os.getcwd(), 'data', 'processed', 'trans_data.pickle'), 'wb')\n",
    "pickle.dump(trans_data, pickle_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#next we'll concatenate all the dataframes in trans_data into a single dataframe\n",
    "merged_data = pd.concat([trans_data[year] for year in trans_data.keys()])\n",
    "pickle_merged = open(os.path.join(os.getcwd(), 'data', 'processed', 'merged_data.pickle'), 'wb')\n",
    "pickle.dump(merged_data, pickle_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
